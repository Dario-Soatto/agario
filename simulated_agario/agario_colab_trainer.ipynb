{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéÆ Agar.io DQN Trainer - Colab Edition\n",
        "\n",
        "Train a reinforcement learning bot for Agar.io using Google Colab's free GPU.\n",
        "\n",
        "## ‚ö° Quick Start\n",
        "1. **Runtime ‚Üí Change runtime type ‚Üí GPU** (for faster training)\n",
        "2. Run all cells in order\n",
        "3. Adjust `NUM_EPISODES` below (5000+ recommended)\n",
        "4. Download `live_model.pth` when done\n",
        "\n",
        "## üìä Expected Performance\n",
        "- **5,000 episodes**: ~10-20 minutes on GPU\n",
        "- **10,000 episodes**: ~20-40 minutes on GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"üî• GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è CONFIGURATION - ADJUST THESE!\n",
        "NUM_EPISODES = 5000   # More = better bot (5000-10000 recommended)\n",
        "SAVE_FREQ = 500       # Save checkpoint every N episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "# Food\n",
        "class Food:\n",
        "    __slots__ = ['x', 'y', 'radius', 'color']\n",
        "    def __init__(self, x, y):\n",
        "        self.x, self.y = x, y\n",
        "        self.radius = random.uniform(4, 8)\n",
        "        self.color = '#FF6B6B'\n",
        "\n",
        "# Bot\n",
        "class Bot:\n",
        "    __slots__ = ['x', 'y', 'radius', 'start_radius', 'color', 'speed', 'behavior', 'alive',\n",
        "                 'direction', 'steps_in_dir', 'steps_per_side', 'wander_angle', 'wander_timer']\n",
        "    def __init__(self, x, y, radius, color, speed=2, behavior='patrol'):\n",
        "        self.x, self.y, self.radius, self.start_radius = x, y, radius, radius\n",
        "        self.color, self.speed, self.behavior, self.alive = color, speed, behavior, True\n",
        "        self.direction, self.steps_in_dir = random.randint(0,3), 0\n",
        "        self.steps_per_side = random.randint(40, 100)\n",
        "        self.wander_angle, self.wander_timer = random.random()*6.28, 0\n",
        "    \n",
        "    def update(self, w, h, px=None, py=None, pr=None):\n",
        "        if not self.alive: return\n",
        "        if self.behavior == 'patrol':\n",
        "            if self.direction == 0: self.x += self.speed\n",
        "            elif self.direction == 1: self.y += self.speed\n",
        "            elif self.direction == 2: self.x -= self.speed\n",
        "            else: self.y -= self.speed\n",
        "            self.steps_in_dir += 1\n",
        "            if self.steps_in_dir >= self.steps_per_side:\n",
        "                self.direction = (self.direction + 1) % 4\n",
        "                self.steps_in_dir = 0\n",
        "        elif self.behavior == 'chase' and px:\n",
        "            if self.radius > pr * 1.1:\n",
        "                dx, dy = px - self.x, py - self.y\n",
        "                d = math.sqrt(dx*dx + dy*dy)\n",
        "                if d > 0 and d < 400:\n",
        "                    self.x += (dx/d) * self.speed\n",
        "                    self.y += (dy/d) * self.speed\n",
        "        elif self.behavior == 'flee' and px:\n",
        "            if pr > self.radius * 1.1:\n",
        "                dx, dy = self.x - px, self.y - py\n",
        "                d = math.sqrt(dx*dx + dy*dy)\n",
        "                if d < 300 and d > 0:\n",
        "                    self.x += (dx/d) * self.speed * 1.2\n",
        "                    self.y += (dy/d) * self.speed * 1.2\n",
        "        elif self.behavior == 'smart' and px:\n",
        "            d = math.sqrt((px-self.x)**2 + (py-self.y)**2)\n",
        "            if self.radius > pr * 1.2 and d < 350:\n",
        "                dx, dy = px - self.x, py - self.y\n",
        "                dd = math.sqrt(dx*dx+dy*dy)\n",
        "                if dd > 0: self.x += (dx/dd)*self.speed; self.y += (dy/dd)*self.speed\n",
        "            elif pr > self.radius * 1.2 and d < 250:\n",
        "                dx, dy = self.x - px, self.y - py\n",
        "                dd = math.sqrt(dx*dx+dy*dy)\n",
        "                if dd > 0: self.x += (dx/dd)*self.speed*1.3; self.y += (dy/dd)*self.speed*1.3\n",
        "        self.x = max(self.radius, min(w - self.radius, self.x))\n",
        "        self.y = max(self.radius, min(h - self.radius, self.y))\n",
        "    \n",
        "    def reset(self, x, y):\n",
        "        self.x, self.y, self.radius, self.alive = x, y, self.start_radius, True\n",
        "        self.direction, self.steps_in_dir = random.randint(0,3), 0\n",
        "\n",
        "# Player\n",
        "class Player:\n",
        "    __slots__ = ['x', 'y', 'radius', 'start_radius', 'color', 'alive', '_score']\n",
        "    def __init__(self, x, y, radius=15):\n",
        "        self.x, self.y, self.radius, self.start_radius = x, y, radius, radius\n",
        "        self.color, self.alive, self._score = '#3498DB', True, radius\n",
        "    @property\n",
        "    def score(self): return self._score\n",
        "    def move(self, dx, dy, w, h):\n",
        "        self.x = max(self.radius, min(w - self.radius, self.x + dx))\n",
        "        self.y = max(self.radius, min(h - self.radius, self.y + dy))\n",
        "    def grow(self, amt):\n",
        "        self.radius = math.sqrt(self.radius**2 + amt * 0.318)\n",
        "        self._score = int(self.radius)\n",
        "    def reset(self, x, y):\n",
        "        self.x, self.y, self.radius, self.alive = x, y, self.start_radius, True\n",
        "        self._score = int(self.start_radius)\n",
        "\n",
        "# Environment\n",
        "class AgarioEnv:\n",
        "    W, H = 3000, 2000\n",
        "    def __init__(self, num_food=150):\n",
        "        self.width, self.height, self.num_food = self.W, self.H, num_food\n",
        "        self.player, self.bots, self.food = None, [], []\n",
        "        self.action_dim, self.state_dim, self.move_speed = 8, 24, 10\n",
        "        d = 1/math.sqrt(2)\n",
        "        self.dirs = [(0,-1),(0,1),(-1,0),(1,0),(d,-d),(d,d),(-d,-d),(-d,d)]\n",
        "        self.steps, self.episode, self.kills, self.deaths, self.last_score = 0, 0, 0, 0, 0\n",
        "        self.reset()\n",
        "    \n",
        "    def _spawn_food(self):\n",
        "        self.food = [Food(random.uniform(20,self.width-20), random.uniform(20,self.height-20)) for _ in range(self.num_food)]\n",
        "    \n",
        "    def _spawn_bots(self):\n",
        "        self.bots = []\n",
        "        cfgs = [(1,(8,12),['flee'],['#98D8C8'],(3.5,5)), (1,(12,18),['flee','patrol'],['#2ECC71'],(3,4.5)),\n",
        "                (1,(20,35),['smart','chase'],['#F39C12'],(2.5,3.5)), (1,(40,60),['chase','smart'],['#E74C3C'],(2,3)),\n",
        "                (1,(70,90),['patrol','smart'],['#8E44AD'],(1.5,2.5))]\n",
        "        px, py = self.width*0.2, self.height*0.5\n",
        "        for cnt, sz, beh, col, spd in cfgs:\n",
        "            for _ in range(cnt):\n",
        "                for _ in range(20):\n",
        "                    x, y = random.uniform(100,self.width-100), random.uniform(100,self.height-100)\n",
        "                    if (x-px)**2 + (y-py)**2 > 40000: break\n",
        "                self.bots.append(Bot(x, y, random.uniform(*sz), random.choice(col), random.uniform(*spd), random.choice(beh)))\n",
        "    \n",
        "    def _get_state(self):\n",
        "        v = []\n",
        "        if self.player.alive: v.extend([self.player.x, self.player.y, self.player.radius])\n",
        "        else: v.extend([-1,-1,-1])\n",
        "        v.append(self.player.score if self.player.alive else 0)\n",
        "        for _ in range(3): v.extend([-1,-1,-1])\n",
        "        if self.player.alive:\n",
        "            px, py = self.player.x, self.player.y\n",
        "            dists = [(((b.x-px)**2+(b.y-py)**2), b.x, b.y, b.radius) for b in self.bots if b.alive]\n",
        "            dists.sort()\n",
        "            for i in range(3):\n",
        "                if i < len(dists): v.extend([dists[i][1], dists[i][2], dists[i][3]])\n",
        "                else: v.extend([-1,-1,-1])\n",
        "        else:\n",
        "            for _ in range(3): v.extend([-1,-1,-1])\n",
        "        v.extend([0, 1 if not self.player.alive else 0])\n",
        "        return np.array(v, dtype=np.float32)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.episode += 1; self.steps = 0\n",
        "        self.player = Player(self.width*0.2, self.height*0.5, 15)\n",
        "        self.last_score = self.player.score\n",
        "        self._spawn_bots(); self._spawn_food()\n",
        "        return self._get_state()\n",
        "    \n",
        "    def step(self, action):\n",
        "        self.steps += 1; reward, done = 0, False\n",
        "        dx, dy = self.dirs[action]\n",
        "        self.player.move(dx*self.move_speed, dy*self.move_speed, self.width, self.height)\n",
        "        px, py, pr = self.player.x, self.player.y, self.player.radius\n",
        "        for b in self.bots: b.update(self.width, self.height, px, py, pr)\n",
        "        rm = []\n",
        "        cd = (pr*0.75)**2\n",
        "        for i, f in enumerate(self.food):\n",
        "            if (f.x-px)**2 + (f.y-py)**2 < cd:\n",
        "                rm.append(i); self.player.grow(f.radius**2*3); reward += 0.5\n",
        "        for i in reversed(rm): self.food.pop(i)\n",
        "        while len(self.food) < self.num_food:\n",
        "            self.food.append(Food(random.uniform(20,self.width-20), random.uniform(20,self.height-20)))\n",
        "        pr = self.player.radius\n",
        "        for b in self.bots:\n",
        "            if not b.alive: continue\n",
        "            dsq = (b.x-px)**2 + (b.y-py)**2\n",
        "            cd = max(pr,b.radius)*0.75\n",
        "            if dsq < cd*cd:\n",
        "                if pr > b.radius*1.1:\n",
        "                    reward += 10+b.radius*0.2; self.player.grow(b.radius**2); b.alive = False; self.kills += 1\n",
        "                elif b.radius > pr*1.1:\n",
        "                    reward -= 100; done = True; self.player.alive = False; self.deaths += 1; break\n",
        "        for b in self.bots:\n",
        "            if not b.alive:\n",
        "                for _ in range(8):\n",
        "                    x, y = random.uniform(100,self.width-100), random.uniform(100,self.height-100)\n",
        "                    if (x-px)**2+(y-py)**2 > 62500: break\n",
        "                b.reset(x, y); b.radius = random.uniform(10, max(50,pr*0.7)); b.start_radius = b.radius\n",
        "        reward += (self.player.score - self.last_score)*0.3\n",
        "        self.last_score = self.player.score\n",
        "        if not done:\n",
        "            reward += 0.05\n",
        "            for b in self.bots:\n",
        "                if b.alive and b.radius > pr*1.1:\n",
        "                    d = math.sqrt((px-b.x)**2+(py-b.y)**2)\n",
        "                    if d < b.radius*3: reward -= (1-d/(b.radius*3))\n",
        "        return self._get_state(), reward, done, {'score': self.player.score}\n",
        "\n",
        "# Q-Network\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, s, a, h=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(s,h), nn.ReLU(), nn.Linear(h,h), nn.ReLU(), nn.Linear(h,a))\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "# Replay Buffer\n",
        "class RLBuf:\n",
        "    def __init__(self, cap=10000): self.buf = deque(maxlen=cap)\n",
        "    def push(self, *args): self.buf.append(args)\n",
        "    def sample(self, n):\n",
        "        b = random.sample(self.buf, min(n, len(self.buf)))\n",
        "        return [np.array(x) for x in zip(*b)]\n",
        "    def __len__(self): return len(self.buf)\n",
        "\n",
        "# DQN Agent\n",
        "class DQN:\n",
        "    def __init__(self, s, a, lr=0.001, g=0.99, eps=1.0, eps_min=0.1, eps_d=0.995, buf=10000, bs=32, tuf=100):\n",
        "        self.a_dim, self.g, self.bs, self.tuf = a, g, bs, tuf\n",
        "        self.eps, self.eps_min, self.eps_d = eps, eps_min, eps_d\n",
        "        self.dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.q = QNet(s,a).to(self.dev); self.qt = QNet(s,a).to(self.dev)\n",
        "        self.qt.load_state_dict(self.q.state_dict())\n",
        "        self.opt = optim.Adam(self.q.parameters(), lr=lr)\n",
        "        self.buf = RLBuf(buf); self.steps, self.eps_cnt = 0, 0; self.rewards = []\n",
        "    \n",
        "    def act(self, s, train=True):\n",
        "        if train and random.random() < self.eps: return random.randint(0, self.a_dim-1)\n",
        "        with torch.no_grad(): return self.q(torch.FloatTensor(s).unsqueeze(0).to(self.dev)).argmax().item()\n",
        "    \n",
        "    def store(self, *args): self.buf.push(*args)\n",
        "    \n",
        "    def learn(self):\n",
        "        if len(self.buf) < self.bs: return\n",
        "        s, a, r, ns, d = self.buf.sample(self.bs)\n",
        "        s = torch.FloatTensor(s).to(self.dev); a = torch.LongTensor(a).to(self.dev)\n",
        "        r = torch.FloatTensor(r).to(self.dev); ns = torch.FloatTensor(ns).to(self.dev)\n",
        "        d = torch.FloatTensor(d).to(self.dev)\n",
        "        cq = self.q(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
        "        with torch.no_grad(): tq = r + self.g * self.qt(ns).max(1)[0] * (1-d)\n",
        "        loss = nn.MSELoss()(cq, tq)\n",
        "        self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
        "        self.steps += 1\n",
        "        if self.steps % self.tuf == 0: self.qt.load_state_dict(self.q.state_dict())\n",
        "    \n",
        "    def end_ep(self, rew): self.eps_cnt += 1; self.rewards.append(rew); self.eps = max(self.eps_min, self.eps*self.eps_d)\n",
        "    \n",
        "    def save(self, p='dqn.pth'):\n",
        "        torch.save({'q_network': self.q.state_dict(), 'target_network': self.qt.state_dict(), \n",
        "                    'optimizer': self.opt.state_dict(), 'epsilon': self.eps, 'steps': self.steps}, p)\n",
        "        print(f\"‚úÖ Model saved: {p}\")\n",
        "\n",
        "print(\"‚úÖ All classes loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ RUN TRAINING\n",
        "def train_dqn(n_eps, save_freq=500):\n",
        "    print('='*60)\n",
        "    print('üéÆ DQN TRAINING - ULTRA FAST MODE')\n",
        "    print('='*60)\n",
        "    env = AgarioEnv()\n",
        "    agent = DQN(env.state_dim, env.action_dim)\n",
        "    print(f'Device: {agent.dev} | Episodes: {n_eps}')\n",
        "    print('='*60)\n",
        "    \n",
        "    t0 = time.time()\n",
        "    rews, lens, best = [], [], -1e9\n",
        "    lt, le = t0, 0\n",
        "    \n",
        "    for ep in range(n_eps):\n",
        "        s, er, st = env.reset(), 0, 0\n",
        "        while True:\n",
        "            st += 1\n",
        "            a = agent.act(s)\n",
        "            ns, r, d, info = env.step(a)\n",
        "            agent.store(s, a, r, ns, float(d))\n",
        "            agent.learn()\n",
        "            er += r\n",
        "            s = ns\n",
        "            if d: break\n",
        "        \n",
        "        agent.end_ep(er)\n",
        "        rews.append(er)\n",
        "        lens.append(st)\n",
        "        \n",
        "        if (ep+1) % 50 == 0:\n",
        "            now = time.time()\n",
        "            dt = now - lt\n",
        "            de = ep+1-le\n",
        "            a10 = np.mean(rews[-10:])\n",
        "            a100 = np.mean(rews[-100:]) if len(rews)>=100 else a10\n",
        "            spd = de/dt if dt > 0 else 0\n",
        "            print(f'Ep {ep+1:5d}/{n_eps} | Rew:{er:6.1f} | Avg100:{a100:6.1f} | Eps:{agent.eps:.3f} | Len:{np.mean(lens[-50:]):.0f} | Spd:{spd:.1f}ep/s | K/D:{env.kills}/{env.deaths}')\n",
        "            lt, le = now, ep+1\n",
        "            if a100 > best and len(rews)>=100:\n",
        "                best = a100\n",
        "                agent.save('dqn_best.pth')\n",
        "        \n",
        "        if (ep+1) % save_freq == 0:\n",
        "            agent.save(f'dqn_ep{ep+1}.pth')\n",
        "    \n",
        "    agent.save('dqn_final.pth')\n",
        "    agent.save('live_model.pth')\n",
        "    \n",
        "    total_time = time.time() - t0\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'‚úÖ TRAINING COMPLETE!')\n",
        "    print(f'‚è±Ô∏è  Time: {total_time/60:.1f} minutes')\n",
        "    print(f'üèÜ Best Avg100: {best:.1f}')\n",
        "    print(f'‚öîÔ∏è  Total K/D: {env.kills}/{env.deaths}')\n",
        "    print(f'{\"=\"*60}')\n",
        "    return agent\n",
        "\n",
        "# Start training!\n",
        "agent = train_dqn(NUM_EPISODES, SAVE_FREQ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì• DOWNLOAD TRAINED MODELS\n",
        "# Run this cell to download your trained model!\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"üì• Downloading models...\")\n",
        "    files.download('live_model.pth')\n",
        "    files.download('dqn_best.pth')\n",
        "    print(\"‚úÖ Download started! Check your browser downloads.\")\n",
        "except:\n",
        "    print(\"üíæ Models saved locally:\")\n",
        "    print(\"  - live_model.pth (use this for live game)\")\n",
        "    print(\"  - dqn_best.pth (best performing model)\")\n",
        "    print(\"  - dqn_final.pth (final model)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Using Your Trained Model\n",
        "\n",
        "The trained model (`live_model.pth`) is **cross-compatible** with the live Agar.io game!\n",
        "\n",
        "### State Space (24 dimensions)\n",
        "```\n",
        "[0-2]   Player: x, y, radius\n",
        "[3]     Score\n",
        "[4-12]  3 nearest viruses (x, y, radius each)\n",
        "[13-21] 3 nearest players (x, y, radius each)\n",
        "[22]    Food count\n",
        "[23]    Game ended flag\n",
        "```\n",
        "\n",
        "### Action Space (8 directions)\n",
        "```\n",
        "0: Up      1: Down    2: Left    3: Right\n",
        "4: Up-Right   5: Down-Right   6: Up-Left   7: Down-Left\n",
        "```\n",
        "\n",
        "### Tips for Better Results\n",
        "- **More episodes = better bot** (try 10,000+)\n",
        "- **Use GPU** for faster training\n",
        "- The bot learns to: avoid larger players, eat smaller ones, grow by eating food\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
